import { command, number, option, positional, run, string } from 'cmd-ts';
import OpenAI from 'openai';
import { config as loadEnv } from 'https://deno.land/x/dotenv@v3.2.2/mod.ts';

// Load environment variables from .env file
await loadEnv({ export: true });

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: Deno.env.get('OPENAI_API_KEY'),
});

interface ProcessingConfig {
  chunkSize: number;
  lookBackSize: number;
  lookForwardSize: number;
  model: string;
}

function countWords(text: string): number {
  return text.trim().split(/\s+/).length;
}

function extractChunk(
  text: string,
  startPos: number,
  length: number,
  config: ProcessingConfig,
): { chunk: string; endPos: number } {
  const words = text.split(/\s+/);
  const chunk = words.slice(startPos, startPos + length).join(' ');
  return { chunk, endPos: startPos + length };
}

function constructPrompt(
  processingPrompt: string,
  lookBack: string,
  currentChunk: string,
  lookForward: string,
  outputLookBack: string,
): string {
  return `You are a natural language processor that works with unusually large inputs and outputs that do not work well in a single request. You will be given a chunk of your input and a task to transform this input chunk to a corresponding output chunk. You will be invoked slowly to build up the final output.

Think of yourself as slowly transforming a large continous input text to another large continues output text using a sliding window, with some look back and forward in your input and look back in our output. Here is the rough diagram:

input:
[-<out of context>-[-<look back, already processed>-[-<chunk being procssed now>-]-<look foward, will be included in next chunk>-]-<out of context>-]
output:
[-<out of context>-[-<look back, already generated in previous run>-]-<unknown, you need to generate this>-<will get generated in the following runs>-]

You can decide to stop generation of the current chunk wherever it makes the most sense, usually end of sentence if the sentece seems to be complete, as long as the cut is close to the end boundary of that chunk, because chunking mechanism is dumb and you need to smoothen the edges to ensure all input is correctly processed. This is one of the reasons you are provided with look back / forward mechanisms to do this near the edge gluing corectly. The chunks you output have to be simply concatenatable. In other words the output should be as if the llm just processed the whole input in one go successfully. In reality the output will be constructed by concatenating the output chunks: "".join(output_chunks).

Always when starting a new chunk response, think: will simply appending new output after appending as is to the output look back already generated in the previous chunk make sense? Remember, the previous chunk was generated by the same instructions, so assume you did the right thing in the previous chunk.

Output exactly the next chunk of the output, to be concatenated to the output look back already generated in the previous chunk, do not include any other text.

<task>${processingPrompt}</task>

<input-look-back>${lookBack}</input-look-back>

<input-chunk-to-process>${currentChunk}</input-chunk-to-process>

<input-look-forward>${lookForward}</input-look-forward>

<output-look-back>${outputLookBack}</output-look-back>`;
}

async function processChunk(
  prompt: string,
  config: ProcessingConfig,
): Promise<string> {
  const completion = await openai.chat.completions.create({
    messages: [{ role: 'system', content: prompt }],
    model: config.model,
    // Not supported by thinking models
    // temperature: 0,
  });

  return completion.choices[0].message.content || '';
}

async function processFile(inputFile: string, prompt: string, config: ProcessingConfig) {
  try {
    const content = await Deno.readTextFile(inputFile);
    console.log(`Processing file: ${inputFile}`);

    let currentPos = 0;
    let outputText = '';
    const totalWords = countWords(content);

    // Prepare output file name
    const lastDotIndex = inputFile.lastIndexOf('.');
    const outputFile = lastDotIndex !== -1
      ? `${inputFile.slice(0, lastDotIndex)}.transformed${inputFile.slice(lastDotIndex)}`
      : `${inputFile}.transformed`;

    while (currentPos < totalWords) {
      // Extract look-back text
      const lookBackStart = Math.max(0, currentPos - config.lookBackSize);
      const lookBack = currentPos > 0
        ? extractChunk(content, lookBackStart, config.lookBackSize, config).chunk
        : '';

      // Extract current chunk
      const { chunk: currentChunk, endPos } = extractChunk(
        content,
        currentPos,
        config.chunkSize,
        config,
      );

      // Extract look-forward text
      const lookForward = endPos < totalWords
        ? extractChunk(content, endPos, config.lookForwardSize, config).chunk
        : '';

      // Get the last part of the output as look-back
      const outputLookBack = outputText.split(/\s+/).slice(-config.lookBackSize).join(' ');

      // Construct the prompt
      const fullPrompt = constructPrompt(
        prompt,
        lookBack,
        currentChunk,
        lookForward,
        outputLookBack,
      );

      // Process the chunk
      const chunkOutput = await processChunk(fullPrompt, config);
      outputText += (outputText ? ' ' : '') + chunkOutput;

      // Write partial results to file
      await Deno.writeTextFile(outputFile, outputText);

      // Move to next chunk
      currentPos = endPos;

      // Log progress
      console.log(
        `Processed ${
          Math.min(endPos, totalWords)
        }/${totalWords} words. Partial results written to ${outputFile}`,
      );
    }

    console.log(`Processing complete. Final output in: ${outputFile}`);
    return outputText;
  } catch (error: unknown) {
    if (error instanceof Error) {
      console.error(`Error processing file: ${error.message}`);
    } else {
      console.error('An unknown error occurred');
    }
    Deno.exit(1);
  }
}

const cmd = command({
  name: 'llm-long',
  description: 'Process long text files with LLM',
  version: '1.0.0',
  args: {
    inputFile: positional({
      type: string,
      displayName: 'input-file',
      description: 'Input file to process',
    }),
    prompt: option({
      type: string,
      long: 'prompt',
      description: 'Verbatim prompt text',
      env: 'LLM_PROMPT',
      defaultValue: () => '',
    }),
    promptFile: option({
      type: string,
      long: 'prompt-file',
      description: 'Path to prompt file',
      env: 'LLM_PROMPT_FILE',
      defaultValue: () => '',
    }),
    chunkSize: option({
      type: number,
      long: 'chunk-size',
      description: 'Size of each chunk in words',
      defaultValue: () => 500,
    }),
    lookBackSize: option({
      type: number,
      long: 'look-back',
      description: 'Size of look-back window in words',
      defaultValue: () => 500,
    }),
    lookForwardSize: option({
      type: number,
      long: 'look-forward',
      description: 'Size of look-forward window in words',
      defaultValue: () => 500,
    }),
    model: option({
      type: string,
      long: 'model',
      description: 'OpenAI model to use',
      // defaultValue: () => 'o3-mini',
      // defaultValue: () => 'gpt-4o',
      defaultValue: () => 'o1',
    }),
  },
  handler: async (args) => {
    let promptText: string;
    if (args.prompt) {
      promptText = args.prompt;
    } else if (args.promptFile) {
      try {
        promptText = await Deno.readTextFile(args.promptFile);
      } catch (error) {
        console.error(
          'Error reading prompt file:',
          error instanceof Error ? error.message : 'Unknown error',
        );
        Deno.exit(1);
      }
    } else {
      console.error('Either --prompt or --prompt-file must be provided');
      Deno.exit(1);
    }

    const config: ProcessingConfig = {
      chunkSize: args.chunkSize,
      lookBackSize: args.lookBackSize,
      lookForwardSize: args.lookForwardSize,
      model: args.model,
    };

    await processFile(args.inputFile, promptText, config);
  },
});

await run(cmd, Deno.args);
